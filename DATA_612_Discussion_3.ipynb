{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Gillian McGovern"
      ],
      "metadata": {
        "id": "gMDCcASLWekG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommender system algorithms are designed to maximize engagement. Like any company, such as a TV network, recommender systems are meant to keep the users engaged with the product as long as possible. Focusing purely on this goal unfortunately reinforces human bias. Society is biased, so recommender systems modeled on user behavior are inherently biased as well.\n",
        "\n",
        "One interesting example that many of us have seen or experienced in real life is gender bias. Evan Estola mentioned on the Software Misadventures podcast that Meetup experienced an issue where an organizer of a meet up group got offended by the company’s suggested topics. This person was looking to lead a meet up group for women in business networking group, and when creating the meetup, the suggested topics were fashion, shopping and other stereotypical topics. The problem with the algorithm was that it used user preferences for the model. So, the data showed that the users that picked women business owners, also picked fashion and shopping topics. Yet, groups that started women in business were not interested in these topics. They had to make an update to the model to focus on group topics and not user.  \n",
        "\n",
        "To maximize engagement, recommender systems focus on the wants of the users and not the “shoulds”. For Netflix, users will end up returning to the usual bingeable popular show instead of the new and different movie that’s been on the user’s list forever. This human behavior is continuously pushed by Netflix when recommending shows based off what users have watched. Similarly, promoting generic most popular shows reinforces human bias, ignoring individual preferences.\n",
        "\n",
        "Additionally, an issue with recommender systems could be that they do not have diverse enough data to train with. If the model does not see diverse data, it cannot make the most accurate ethical suggestions for users. The algorithm has to work for everybody.\n",
        "\n",
        "Even though certain problematic societal factual trends exist in data, it does not mean that the model should perpetuate these trends. It’s impossible to get every prediction exactly ethically correct, but there are ways to try to ensure models are putting people and ethics first. There are 3rd party auditing companies that will investigate a company's algorithm code to ensure that there is no hidden algorithmic discrimination. Although, enforcing accountability on the company is not guaranteed even with that. Using auditing shouldn’t be a PR stunt by the recommender system company. Additionally, regulations for the auditing process still need to be standardized as well.\n",
        "\n",
        "Recommender companies can do the work of removing societal bias data from the model on their own by performing diverse testing. It is important for companies to ensure they are covering as many test scenarios as possible, and learning which features are the most sensitive.\n",
        "\n",
        "Overall, human bias in recommender systems exists and is an important topic to be aware of going forward. It’s critical to do the best we can in mitigating algorithms learning societal bias.\n",
        "\n",
        "\n",
        "\n",
        "Other resources used:\n",
        "\n",
        "https://www.youtube.com/watch?v=quFefX9KSyI\n",
        "\n",
        "https://themarkup.org/the-breakdown/2021/02/23/can-auditing-eliminate-bias-from-algorithms"
      ],
      "metadata": {
        "id": "axKAhxnjWglu"
      }
    }
  ]
}